{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a923308",
   "metadata": {},
   "source": [
    "#Thanks to: https://pythonology.eu/a-rag-web-scraper-with-langchain-ollama-and-chroma/\n",
    "\n",
    "Getting repo readme context, applied to:\n",
    "\n",
    "1. https://github.com/JAlcocerT/Streamlit-MultiChat\n",
    "2. https://github.com/qatrackplus/qatrackplus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc604c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requests\n",
    "# !pip install beautifulsoup4\n",
    "# !pip install langchain-community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9855310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.3.25\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/jalcocert/Desktop/IT/Data-Chat/.venv/lib/python3.10/site-packages\n",
      "Requires: async-timeout, langchain-core, langchain-text-splitters, langsmith, pydantic, PyYAML, requests, SQLAlchemy\n",
      "Required-by: langchain-community\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43006282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#brings your OPENAI_API_KEY\n",
    "!source .env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f80bbee",
   "metadata": {},
   "source": [
    "In this step, weâ€™re using **BeautifulSoupâ€™s SoupStrainer** to focus on the â€œcontent-areaâ€ class, ensuring we only extract relevant information. \n",
    "\n",
    "The WebBaseLoader then fetches and processes the specified URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "231c5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class=\"markdown-body entry-content container-lg\"\n",
    "\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"markdown-body entry-content container-lg\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://github.com/JAlcocerT/Streamlit-MultiChat\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b5e7825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-text-splitters\n",
      "Version: 0.3.8\n",
      "Summary: LangChain text splitting utilities\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/jalcocert/Desktop/IT/Data-Chat/.venv/lib/python3.10/site-packages\n",
      "Requires: langchain-core\n",
      "Required-by: langchain\n"
     ]
    }
   ],
   "source": [
    "!pip show langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2925302",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200, chunk_overlap=100, add_start_index=True\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dca5f9c",
   "metadata": {},
   "source": [
    "Get Ollama ready:\n",
    "\n",
    "* https://github.com/JAlcocerT/Docker/tree/main/AI_Gen/Ollama\n",
    "\n",
    "\n",
    "```sh\n",
    "sudo docker stats ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e743559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !docker exec -it ollama sh\n",
    "#hostname -I \n",
    "#--->> 172.20.0.2 \n",
    "#sudo docker inspect ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85a49265",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama run llama3.2:1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77591302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama pull all-minilm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ffb917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-chroma\n",
    "# #https://pypi.org/project/langchain-chroma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e81d88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-chroma\n",
      "Version: 0.2.3\n",
      "Summary: An integration package connecting Chroma and LangChain\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/jalcocert/Desktop/IT/Data-Chat/.venv/lib/python3.10/site-packages\n",
      "Requires: chromadb, langchain-core, numpy\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27a93646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_535655/1148073819.py:10: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  local_embeddings = OllamaEmbeddings(model=\"all-minilm\", base_url=\"http://0.0.0.0:11434\")\n"
     ]
    }
   ],
   "source": [
    "###DEPRICATED VERSION OF DOING IT###\n",
    "\n",
    "# from langchain_community.embeddings import OllamaEmbeddings #https://python.langchain.com/docs/integrations/text_embedding/ollama/\n",
    "# from langchain_chroma import Chroma\n",
    "\n",
    "# # local_embeddings = OllamaEmbeddings(model=\"all-minilm\")\n",
    "# # vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings)\n",
    "\n",
    "# from langchain_community.embeddings import OllamaEmbeddings\n",
    "# #local_embeddings = OllamaEmbeddings(model=\"all-minilm\", base_url=\"http://192.168.0.12:11434\")\n",
    "# #local_embeddings = OllamaEmbeddings(model=\"all-minilm\", base_url=\"http://192.168.1.5:11434\")\n",
    "# local_embeddings = OllamaEmbeddings(model=\"all-minilm\", base_url=\"http://0.0.0.0:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7825019e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #https://pypi.org/project/langchain-ollama/\n",
    "# !pip install langchain-ollama==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44a044df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-ollama\n",
      "Version: 0.3.2\n",
      "Summary: An integration package connecting Ollama and LangChain\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/jalcocert/Desktop/IT/Data-Chat/.venv/lib/python3.10/site-packages\n",
      "Requires: langchain-core, ollama\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5572b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "#Using a local embedding model\n",
    "#https://ollama.com/library/all-minilm\n",
    "\n",
    "# local_embeddings = OllamaEmbeddings(model=\"all-minilm\")\n",
    "# vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings)\n",
    "\n",
    "# local_embeddings = OllamaEmbeddings(model=\"all-minilm\", base_url=\"http://192.168.0.12:11434\")\n",
    "# local_embeddings = OllamaEmbeddings(model=\"all-minilm\", base_url=\"http://192.168.1.5:11434\")\n",
    "local_embeddings = OllamaEmbeddings(model=\"all-minilm\", base_url=\"http://0.0.0.0:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06a073ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=local_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9635f599",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is the repository about?\"\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retrieved_docs = retriever.invoke(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c541ae61",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ' '.join([doc.page_content for doc in retrieved_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87472e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Streamlit-MultiChat\\n\\n\\nMany LLMs - One Streamlit Web App\\n\\n\\nOpenAI | Anthropic | Ollama | Groq\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nA custom Streamlit Web App to Chat with the latest LLMs and get a per use cost instead of a fixed monthly price.\\nFeatures\\nUse many large language models: OpenAI, Anthropic, Open / Local LLM\\'s with one Streamlit Web App.\\n\\nLLM Support\\n\\nOllama - Open Source Models\\nOpenAI - GPT 3.5 / GPT4 / GPT4o / GPT4o-mini\\nAnthropic - Claude 3 (Opus / Sonnet) / Claude 3.5\\nGroq API - LlaMa models using quick LPU inference\\n\\n\\nExtended explanation\\n\\nSliDev presentation of the Streamlit-MultiChat\\nThis blog post â†’\\nDeploy as per https://github.com/JAlcocerT/Streamlit-MultiChat/tree/main/Z_DeployMe\\n\\n\\n\\nDuring the process, I also explored: SliDev PPTs, ScrapeGraph, DaLLe, Streamlit Auth and OpenAI as Custom Agents\\nGetting Started\\nThe Project is documented here â†’\\n\\nClone the repository and Run with your API keys ðŸ‘‡\\n  \\xa0\\n\\nOpenAI API Keys - https://platform.openai.com/api-keys\\nAnthropic - https://console.anthropic.com/settings/keys\\nGroq - https://console.groq.com/keys\\nFor Ollama, you need this setup\\n\\nTry the Project quickly with Python Venv\\'s:\\n\\nGet Python Installed\\nPrepare a Venv You will need Docker ready. And optionally Portainer\\n\\n\\nThanks to â¤ï¸\\nProjects I got inspiration from / consolidated in this App were tested here: ./Z_Tests\\n\\nCheck the Projects ðŸ‘ˆ\\n  \\xa0\\n\\n\\nhttps://github.com/dataprofessor/openai-chatbot\\n\\n\\nhttps://github.com/AIDevBytes/Streamlit-Ollama-Chatbot\\n\\n\\nhttps://github.com/tonykipkemboi/groq_streamlit_demo -> Groq + Streamlit Chat\\n\\n\\nhttps://github.com/TirendazAcademy/Streamlit-Tutorials/blob/main/Blog-Generator-App-with-Claude-API/app.py\\n\\nhttps://www.youtube.com/watch?v=ximj9QWle-g\\n\\n\\n\\nhttps://github.com/siddhardhan23/gemini-pro-streamlit-chatbot Try the Project quickly with Python Venv\\'s:\\n\\nGet Python Installed\\nPrepare a Venv\\n\\ngit clone https://github.com/JAlcocerT/Streamlit-MultiChat\\n#python -m venv multichat_venv #create the venv\\npython3 -m venv multichat_venv #linux\\n\\n#multichat_venv\\\\Scripts\\\\activate #activate venv (windows)\\nsource multichat_venv/bin/activate #(linux)\\nThen, provide the API Keys and run the Streamlit Web App:\\n#uv pip install -r requirements.txt\\npip install -r requirements.txt #all at once, ~2min\\n\\ncp ./.streamlit/secrets_sample.toml ./.streamlit/secrets.toml #fill the API Keys\\nstreamlit run Z_multichat.py\\n\\nMake sure to have Ollama ready and running your desired model!\\nPrepare the API Keys in any of:\\n\\n.streamlit/secrets.toml\\nAs Environment Variables\\n\\nLinux - export OPENAI_API_KEY=\"YOUR_API_KEY\"\\nCMD - set OPENAI_API_KEY=YOUR_API_KEY\\nPS - $env:OPENAI_API_KEY=\"YOUR_API_KEY\"\\nIn the Docker-Compose\\n\\n\\nThrough the Streamlit UI\\n\\n\\n\\n\\n\\n\\nChat with Several Models with Streamlit\\n\\n\\nAlternatively - Use the Docker Image\\n\\ndocker pull ghcr.io/jalcocert/streamlit-multichat:latest #x86/ARM64\\n\\nYou will need Docker ready. And optionally Portainer'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8864686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U langchain-ollama\n",
    "#0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3586e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://python.langchain.com/api_reference/ollama/index.html\n",
    "\n",
    "# from langchain_ollama.llms import OllamaLLM\n",
    "# llm = OllamaLLM(model=\"llama3.2:1b\")\n",
    "\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "#!ollama pull llama3.2:1b\n",
    "#!ollama list\n",
    "llm = OllamaLLM(model=\"llama3.2:1b\", base_url=\"http://0.0.0.0:11434\")\n",
    "#llm = OllamaLLM(model=\"llama3.2:1b\", base_url=\"http://192.168.0.12:11434\") #workin with LLama3.2\n",
    "#llm = OllamaLLM(model=\"all-minilm\", base_url=\"http://0.0.0.0:11434\") #not working with this model, as it is a embedding model!!!\n",
    "\n",
    "\n",
    "response = llm.invoke(f\"\"\"Answer the question according to the context given very briefly:\n",
    "           Question: {question}.\n",
    "           Context: {context}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "070934da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'There are several oversold and overbought periods:\\n\\n- Overbought: The Relative Strength Index (RSI) is above 70 (30-70)\\n- Oversold: The Relative Strength Index (RSI) is below 30 (70-30)\\n\\nThese levels can be used to generate buy or sell signals based on technical analysis strategies.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "586a4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_openai==0.3.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80adc1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export OPENAI_API_KEY=\"your-api-key\"\n",
    "#source .env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1240af03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available OpenAI Models:\n",
      "- gpt-4o-audio-preview-2024-12-17\n",
      "- dall-e-3\n",
      "- dall-e-2\n",
      "- gpt-4o-audio-preview-2024-10-01\n",
      "- gpt-4-turbo-preview\n",
      "- text-embedding-3-small\n",
      "- gpt-4-turbo\n",
      "- gpt-4-turbo-2024-04-09\n",
      "- gpt-4.1-nano\n",
      "- gpt-4.1-nano-2025-04-14\n",
      "- gpt-4o-realtime-preview-2024-10-01\n",
      "- gpt-4o-realtime-preview\n",
      "- babbage-002\n",
      "- gpt-4\n",
      "- text-embedding-ada-002\n",
      "- chatgpt-4o-latest\n",
      "- gpt-4o-realtime-preview-2024-12-17\n",
      "- gpt-4o-mini-audio-preview\n",
      "- gpt-4o-audio-preview\n",
      "- o1-preview-2024-09-12\n",
      "- gpt-4o-mini-realtime-preview\n",
      "- gpt-4.1-mini\n",
      "- gpt-4o-mini-realtime-preview-2024-12-17\n",
      "- gpt-3.5-turbo-instruct-0914\n",
      "- gpt-4o-mini-search-preview\n",
      "- gpt-4.1-mini-2025-04-14\n",
      "- o1\n",
      "- o1-2024-12-17\n",
      "- davinci-002\n",
      "- gpt-3.5-turbo-1106\n",
      "- gpt-4o-search-preview\n",
      "- gpt-4-1106-preview\n",
      "- gpt-3.5-turbo-instruct\n",
      "- gpt-3.5-turbo\n",
      "- gpt-4o-mini-search-preview-2025-03-11\n",
      "- gpt-4-0125-preview\n",
      "- gpt-4o-2024-11-20\n",
      "- whisper-1\n",
      "- gpt-4o-2024-05-13\n",
      "- o1-pro\n",
      "- gpt-3.5-turbo-16k\n",
      "- gpt-image-1\n",
      "- o1-pro-2025-03-19\n",
      "- o1-preview\n",
      "- gpt-4-0613\n",
      "- text-embedding-3-large\n",
      "- gpt-4o-mini-tts\n",
      "- gpt-4o-transcribe\n",
      "- gpt-4.5-preview\n",
      "- gpt-4.5-preview-2025-02-27\n",
      "- gpt-4o-mini-transcribe\n",
      "- gpt-4o-search-preview-2025-03-11\n",
      "- omni-moderation-2024-09-26\n",
      "- o3-mini\n",
      "- o3-mini-2025-01-31\n",
      "- tts-1-hd\n",
      "- gpt-4o\n",
      "- tts-1-hd-1106\n",
      "- gpt-4o-mini\n",
      "- gpt-4o-2024-08-06\n",
      "- gpt-4.1\n",
      "- gpt-4.1-2025-04-14\n",
      "- gpt-4o-mini-2024-07-18\n",
      "- o1-mini\n",
      "- gpt-4o-mini-audio-preview-2024-12-17\n",
      "- gpt-3.5-turbo-0125\n",
      "- o1-mini-2024-09-12\n",
      "- tts-1\n",
      "- tts-1-1106\n",
      "- omni-moderation-latest\n",
      "- o4-mini-2025-04-16\n",
      "- o4-mini\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Configure your OpenAI API key (if not set as environment variable)\n",
    "# openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "\n",
    "try:\n",
    "    models = openai.models.list()\n",
    "    model_names = [model.id for model in models.data]\n",
    "    print(\"Available OpenAI Models:\")\n",
    "    for name in model_names:\n",
    "        print(f\"- {name}\")\n",
    "except openai.error.AuthenticationError as e:\n",
    "    print(f\"Authentication error: {e}\")\n",
    "    print(\"Please ensure your OPENAI_API_KEY environment variable is set correctly.\")\n",
    "except openai.error.OpenAIError as e:\n",
    "    print(f\"An OpenAI API error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "426db8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "\n",
    "# You'll need your OpenAI API key\n",
    "# It can be set as an environment variable OPENAI_API_KEY=your_key\n",
    "# Or passed directly as a parameter:\n",
    "\n",
    "\n",
    "\n",
    "#llm = OpenAI(model_name=\"gpt-3.5-turbo-instruct\") # Or any other OpenAI model\n",
    "llm = OpenAI(model_name=\"gpt-4.1-nano\")\n",
    "\n",
    "response = llm.invoke(f\"\"\"Answer the question according to the context given very briefly:\n",
    "           Question: {question}.\n",
    "           Context: {context}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "176e1a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-openai\n",
      "Version: 0.3.16\n",
      "Summary: An integration package connecting OpenAI and LangChain\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/jalcocert/Desktop/IT/Data-Chat/.venv/lib/python3.10/site-packages\n",
      "Requires: langchain-core, openai, tiktoken\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "20b6f129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. The repository is about a Streamlit web app that allows users to chat with multiple large language models (LLMs) from various providers like OpenAI, Anthropic, Ollama, and Groq, and compare costs on a per-use basis.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677bf99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install groq==0.24.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "00dfe686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: groq\n",
      "Version: 0.24.0\n",
      "Summary: The official Python library for the groq API\n",
      "Home-page: https://github.com/groq/groq-python\n",
      "Author: \n",
      "Author-email: Groq <support@groq.com>\n",
      "License: Apache-2.0\n",
      "Location: /home/jalcocert/Desktop/IT/Data-Chat/.venv/lib/python3.10/site-packages\n",
      "Requires: anyio, distro, httpx, pydantic, sniffio, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f18db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!source .env\n",
    "#export GROQ_API_KEY=\"your-api-key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443e9fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Groq Models:\n",
      "- llama-3.1-8b-instant\n",
      "- allam-2-7b\n",
      "- llama-3.3-70b-versatile\n",
      "- meta-llama/llama-4-scout-17b-16e-instruct\n",
      "- compound-beta\n",
      "- llama3-70b-8192\n",
      "- whisper-large-v3\n",
      "- distil-whisper-large-v3-en\n",
      "- playai-tts\n",
      "- playai-tts-arabic\n",
      "- llama3-8b-8192\n",
      "- qwen-qwq-32b\n",
      "- deepseek-r1-distill-llama-70b\n",
      "- mistral-saba-24b\n",
      "- llama-guard-3-8b\n",
      "- compound-beta-mini\n",
      "- gemma2-9b-it\n",
      "- whisper-large-v3-turbo\n",
      "- meta-llama/llama-4-maverick-17b-128e-instruct\n"
     ]
    }
   ],
   "source": [
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    # Get the Groq API key from the environment variables\n",
    "    groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "    if not groq_api_key:\n",
    "        raise ValueError(\"GROQ_API_KEY not found in environment variables or .env file.\")\n",
    "\n",
    "    # Initialize the Groq client with the API key\n",
    "    groq_client = Groq(api_key=groq_api_key)\n",
    "\n",
    "    models = groq_client.models.list()\n",
    "    model_names = [model.id for model in models.data]\n",
    "    print(\"Available Groq Models:\")\n",
    "    for name in model_names:\n",
    "        print(f\"- {name}\")\n",
    "\n",
    "except ValueError as ve:\n",
    "    print(f\"Configuration Error: {ve}\")\n",
    "    print(\"Please ensure you have a .env file with GROQ_API_KEY set.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while fetching Groq models: {e}\")\n",
    "    print(\"Please ensure you have the 'groq' library installed and your GROQ_API_KEY is correctly configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3c437c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain-groq==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d99586ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-groq\n",
      "Version: 0.3.2\n",
      "Summary: An integration package connecting Groq and LangChain\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /home/jalcocert/Desktop/IT/Data-Chat/.venv/lib/python3.10/site-packages\n",
      "Requires: groq, langchain-core\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e58b5709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The repository is about a custom Streamlit Web App that allows users to chat with various large language models (LLMs) from different providers, including OpenAI, Anthropic, Ollama, and Groq, at a per-use cost instead of a fixed monthly price.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 854, 'total_tokens': 910, 'completion_time': 0.074666667, 'prompt_time': 0.101103705, 'queue_time': 1.122559132, 'total_time': 0.175770372}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f7bd09b454', 'finish_reason': 'stop', 'logprobs': None} id='run--2d780429-828b-4ae7-b5ad-3f05e12fd9cc-0' usage_metadata={'input_tokens': 854, 'output_tokens': 56, 'total_tokens': 910}\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the Groq API key from the environment variables\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not groq_api_key:\n",
    "    raise ValueError(\"GROQ_API_KEY not found in environment variables or .env file.\")\n",
    "\n",
    "# Initialize the Groq chat model\n",
    "llm = ChatGroq(\n",
    "    model_name=\"llama-3.1-8b-instant\",  # Or another available Groq model\n",
    "    groq_api_key=groq_api_key\n",
    ")\n",
    "\n",
    "# Example usage (assuming 'question' and 'context' variables are defined)\n",
    "#question = \"What is the capital of Poland?\"\n",
    "#context = \"Warsaw is the largest city and capital of Poland.\"\n",
    "\n",
    "question = \"what is the repository about?\"\n",
    "context = ' '.join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "\n",
    "try:\n",
    "    response = llm.invoke(f\"\"\"Answer the question according to the context given very briefly:\n",
    "               Question: {question}.\n",
    "               Context: {context}\"\"\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during inference: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e7966bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "99bf0626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The repository is about a custom Streamlit Web App that allows users to chat with various large language models (LLMs) from different providers, including OpenAI, Anthropic, Ollama, and Groq, at a per-use cost instead of a fixed monthly price.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)  # Access the generated text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "15e46379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip freeze > requirements-export.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
